# Third week: improve finetune and RL (GRPO) 

## API choice

- **Backbone**: Qwen3-1.7B
- **Finetune Structure**: LLaMA Factory, Lora
- **RL Structure**: open-r1, GRPO

## System prompt (For both finetune and RL, I just provide with a general system prompt, not specific improvement method prompt)

**COMMON_SYSTEM_PROMPT:** You are an expert ml researcher having previous background in SEO and search engines in general. You are working on novel research ideas for next generation of products. These products will have language models augmented with search engines, with the task of answering questions based on sources backed by the search engine. This new set of systems will be collectively called language engines (generative search engines). This will require websites to update their SEO techniques to rank higher in the llm generated answer. Specifically they will use GEO (Generative Engine Optimization) techniques to boost their visibility in the final text answer outputted by the Language Engine.  

**COMMON_USER_PROMPT_START:** General Instruction: I will give you a source of website source. The source along with other sources. will be used for answering some question posed by the user. The answer will be generated by LLM using these multiple sources, and each of the lines will be cited by Language model. As the owner of the source, the task is to increase your visibility in the answer. To do this you will appropriately change the text of the source (without changing the content) so that it is ranked higher in terms of impact in the final answer.\n\n

## New key-point evaluation metrics and evaluations

**For key-point evaluation, considering its difference with original webpage, two designs are proposed:**

**old_vs_new.** Evaluate the alignment between the key point extracted from the original text and the improved text.

**new_vs_old.** Evaluate the alignment between the key point extracted from the improved text and the original text.

**results:**
![Gemini GEO Table](./pic/keypoint_pre.png)

**analysis:**

1. Whether it is the support rate for the rewritten webpage contentâ€™s key points found in the original webpage, or vice versa, and regardless of the regeneration prompt used, the support rate is almost always above 80%, while the opposition rate remains very low. This indicates that our regenerated content still preserves the main viewpoints of the original content.
2. Method "Better Formulations" (new) behaves the best in keeping the viewpoint of the original web content.

## Filtering for finetune strategy

**1.** All of objective metrics should be higher after regenerating.

**2.** At least 4 of 7 subjective metrics should be higher after regenerating.

**3.** The support rate for both keypoint metrics should be greater than 0.8, while the contradict count should be 0.

**Final sample num.** About 2000 samples.

## Results for finetune and RL

**finetune training loss:**

![Gemini GEO Table](./pic/finetune_4epoch_loss.png)

**finetune inference:**

![Gemini GEO Table](./pic/finetune_results.png)

**RL training rewards:**

![Gemini GEO Table](./pic/RL_100step(200sample)_reward.png)

**RL inference:**

![Gemini GEO Table](./pic/RL_results.png)

**setting explanations:**

**A_B_X(for example: finetune_4_instruct):** B means steps (one step contains 2 samples, one sample have 8 different compeletions for RL to optimize) for RL and means epoch for finetune.

**wen3-instruct:** Use original Qwen-1.7B directly regenerate webpage.

**RL-settings:** 3 objective metrics; weighted: 0.8, 0.1, 0.1; rewards: (new_score * 0.6 + new_pos_score * 0.2 + new_word_score * 0.2)  - (ori_score * 0.6 + ori_pos_score * 0.2 + ori_word_score * 0.2) 

**data choice:** For finetune steps, using filtering data; while for RL steps, using first 5000 data. For inference, use completely new 1000 queires in Researchie Questions.

**results analysis:**

**1.** During the finetuning phase, although after 4 epochs the regeneration performance was not as good as that of the original webpage content, it was still significantly better than directly using Qwen3 for inference. Moreover, increasing the number of epochs further actually led to a decline in inference performance. This indicates that, firstly, our finetuning is indeed effective. However, due to the relatively small size of the Qwen3-1.7B model, it may not be capable of learning how to improve performance through regeneration during finetuning. On the other hand, it is also possible that we only provided a general regeneration prompt during supervised training, without offering specific prompts related to the improvement methods, which limited the model's effectiveness.

**2.** During the RL phase, due to the significant time overhead caused by API calls, we have temporarily not included keypoint evaluation as one of the metrics. Previously, the API was called using a single thread, resulting in slow speed, and under a very small sample setting (100 steps, 200 samples), the performance was not ideal. I have now switched to multi-threaded API calls, which will allow us to evaluate the RL results after completing one full epoch over the entire dataset. Currently, the RL phase also uses the same general prompt as in finetuning, and prompts related to specific improvement methods have not yet been applied.

## Future plans:

**1.** Firstly need to find ways to improve objective metrics in RL steps, and if, after the RL phase, the model achieves significant improvements in objective metrics (aligned with the original work), while still using only a general regeneration prompt, this would represent a major advancement. It would mean that the model does not need to rely on specific improvement method prompts to achieve better performance.

**2.** For the setting involving positive sentiment bias, this metric can be optimized using RL methods. For the setting that requires generating specific content, explicitly including keypoints may be more feasible, and prompt engineering might achieve better results than RL in this case.
