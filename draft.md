# Eighth week: Autorule merge deep dug & Design LLM adjust before finetune & Test RL, finetune for new framework & Draw flowchart for merge process and whole structure till now.

## API choice

- **Backbone**: Qwen3-1.7B, Qwen2.5-7B， gemini-2.5-flash-lite-preview
- **Finetune Structure**: LLaMA Factory, Lora
- **RL Structure**: open-r1, GRPO
- **RAG Generation API**: gpt-4o-mini, gemini-2.5-flash-lite-preview, claude-3-haiku-20240307

## General Abstrct:

### Merge method rewritten

Rewrote the merge method in Autorule. The “reason” step had already been designed for RAG rather than RLHF, so the current Autorule—while still keeping the three-step structure (reason, extract, merge)—is now a fully new version tailored for RAG. After the rewrite, using Autorule alone as the prompt raised the GEO score from 0.33 to 0.39.

### Use LLM to adjust the content of teacher for better finetuning

When using prompts generated by Autorule to regenerate original content into new documents, first apply an LLM to standardize the format. This ensures that, without altering the original structure, the rewritten content always appears immediately after "Rewritten Source: ". Such formatting makes it easier to extract during finetuning and later inference.

### Draw flowchart

To make the process easier to understand and to improve clarity in future research and paper discussions, I created flowcharts for the merge module as well as the entire prompt-engineering, finetuning, and RL workflow, and attached them to this report.

## Something have to be decided this meeting:

### Dataset

I currently use two datasets: GEO-Bench and Researchy Questions. For GEO-Bench, we use the full training and test sets. For Researchy Questions, we use the first 10,000 samples from the training set (10,000 out of 90,000) and the first 2,000 samples from the test set (2,000 out of 6,000). Previously, our test set was simply samples 10,000–12,000 from the original training set, which felt odd, so this week we switched to the actual test set.

This design choice is based on two factors: **Dataset scale alignment** – the size is now roughly comparable to GEO-Bench. **Practical constraints** – the full Researchy Questions dataset is extremely large, making Autorule extraction, Gemini prompt engineering, and RL prohibitively time-consuming and expensive in terms of API usage. Any later adjustments or modifications would also incur a very high time cost.

However, I’m still unsure whether this design is entirely reasonable.

### Autorule

In the current Autorule setup, when extracting rules, the LLM agent is given the query, the best document, and the worst document as input. However, we then filter out any rules that contain the query itself, and use only the remaining rules as our final prompt. The reasoning behind this is somewhat complex and may require further explanation during a meeting. I’m not sure if this design might feel counterintuitive or unusual.

### Prompt for Gemini and Qwen-1.7B

Starting this week, both the RL and Gemini prompt engineering stages have completely stopped using methods from the GEO paper, relying solely on our own rules. This change was made because the improved rules have already increased the GEO score from 0.33 to 0.39–0.40. Even when combining our rules with all GEO paper methods, the score only reaches 0.41. Furthermore, when using only our rules as the prompt for the rewriting task, the final semantic keypoint coverage reaches 0.84—compared to 0.81 when combining GEO paper methods with our rules.

### RAG API

This week, after checking with Zhihan, it appears the only callable APIs we have are Gemini, Claude, and GPT—with no access to Qwen, DeepSeek, or similar models.

Upon examining Qwen-7B in detail for RAG generation, I found its instruction-following ability to be poor: the numeric identifiers appended to each sentence were often corrupted—either out of range or heavily duplicated—making the resulting GEO score calculations highly unreliable. Because of this, I have not included Qwen-7B in our experiments.

Currently, we are running six experiment settings: **APIs:** Gemini, Claude, GPT; **Datasets:** GEO-Bench, Researchy Questions

Finetuning and RL are performed only on Gemini + Researchy Questions, and the RL-trained Qwen-1.7B model is then transferred to other settings for testing. We can discuss in the meeting whether this setup has potential issues or needs adjustments.

### RAG Score Calculation

I reviewed the DeepResearchGym evaluation keypoints and found that only 17 overlap with the query IDs in our test set. In the original DeepResearchGym, keypoint coverage for each query is calculated by aggregating all keypoints from its reference documents before computing coverage. However, their setup works because most of the top 1000 queries they use have complete reference document content retrievable from ClueWeb.

In contrast, our test dataset contains many reference documents for which complete content cannot be retrieved. Therefore, directly applying their aggregation approach may not be appropriate. My idea is to instead compute keypoint coverage per document for each query, then take a click rate–weighted sum across documents.




## Keypoint Rewritten:

### General ideas:

We no longer evaluate keypoints individually. Instead, we provide all keypoints from the original document along with the rewritten document to the GPT model in a single batch, enabling simultaneous evaluation. This approach offers two main advantages:

**1.** With full keypoint context, the model makes more accurate judgments (validated against human annotation or more expensive models).

**2.** It significantly improves the efficiency of keypoint threshold evaluation during the RL phase, reducing overall cost.

### new keypoint prompt:

```text
def create_batch_prompt_judge(simplified_key_points: list, answer: str) -> str:
    """
    Creates a prompt for an LLM to judge ALL key points against a report in a single call.
    The LLM is instructed to return a JSON object mapping point_number to its judgment.

    Args:
        simplified_key_points: A pre-processed list of dictionaries, 
                               each containing 'point_number' and 'point_content'.
        answer: The full report text to judge against.

    Returns:
        A formatted prompt string for the LLM.
    """
    key_points_json_str = json.dumps(simplified_key_points, indent=4, ensure_ascii=False)

    return f"""You are given a **JSON array of Key Points** and a **Report**.

For **each** Key Point in the JSON array, your job is to determine whether the Report:
- **Supported** the Key Point: means the Report contains information that supports the Key Point.
- **Omitted** the Key Point: means the Report does not mention or cover the Key Point.
- **Contradicted** the Key Point: means the Report says something that disagrees with or negates the Key Point.

Carefully read each Key Point and the Report.

Return your answer as a **single JSON object**. The keys of this object must be the `point_number` from the input Key Points, converted to a string. The value for each key must be another JSON object with two fields:
- "label": One of "Supported", "Omitted", or "Contradicted".
- "justification": A brief explanation for your label.

For example, your response should look like this:
{{
  "1": {{
    "label": "Supported",
    "justification": "The report's first section directly defines this term."
  }},
  "2": {{
    "label": "Omitted",
    "justification": "The report discusses data misuse causes but does not mention this specific aspect."
  }}
}}

Respond **only** with the JSON object. Do not add any commentary, text, or markdown formatting like ```json.

---

Key Points:
{key_points_json_str}

---

Report:
{answer}
"""
```

## Keypoint threshold for finetune and RL step

### Finetuning:

Used the updated, more accurate keypoint metric to filter data for the teacher model, and raised the keypoint coverage threshold from 0.7 to 0.85.

### RL:

Two settings were configured with keypoint coverage thresholds set to 0.8 and 0.9, respectively. For all completions with keypoint coverage below the threshold, the objective reward of the rewritten document was set to zero—resulting in a negative minimum value when subtracting the original document's objective reward.

## Autorule contrast for different model and different dataset (the same color represents the similar rules)

### Contrast among different RAG generators (all using Researchy Questions)

### Contrast between GEOBench and Researchy Questions (all using gemini-2.5-flash-lite)


## Experienment Results

### Researchy Questions

### GEOBench

### Ablation Studies for Researchy Questions

**finetune:**

**RL:**


### One novel metric designed for Researchy Question dataset:

To address the subjectivity of the Reseachy Questions dataset, we propose a new metric called Completeness. This metric is designed to measure how thoroughly the RAG response covers the various dimensions of a query. We are able to implement this because the dataset uniquely provides predefined sub-questions and aspects, which essentially serve as a blueprint for a comprehensive answer.

### Prompts for all RAG evaluation metrics:

**Faithfulness:**

```text
"""You are a meticulous fact-checker. Your task is to evaluate if a "Statement" is fully and accurately supported by the "Source Text". Answer on a scale of 1 to 5. Your response MUST begin with a single digit from 1 to 5, followed by a newline and a brief explanation.

- 5: The statement is fully and directly supported by the source text. All claims in the statement can be clearly verified from the source.
- 4: The statement is mostly supported, but might involve a minor, reasonable inference.
- 3: The statement is partially supported, but contains significant information not present in the source.
- 2: The statement is related to the source text's topic but the core claim is not supported.
- 1: The statement is completely unsupported by or contradicts the source text.

Source Text:
"""
{source_document_content}
"""

Statement:
"""
{generated_sentence}
"""

Your Rating (1-5):"""
```

**Relevance:**

```text
"""You are an expert question-answering evaluator. Your task is to rate how relevant the "Generated Answer" is to the "User Query". Answer on a scale of 1 to 5. Your response MUST begin with a single digit from 1 to 5, followed by a newline and a brief explanation.

- 5: The answer is perfectly relevant, directly and completely answering the user's query.
- 4: The answer is highly relevant but may contain minor, slightly off-topic information.
- 3: The answer is moderately relevant, addressing the main topic but failing to answer the specific question asked.
- 2: The answer is only slightly relevant, focusing on a tangent of the query.
- 1: The answer is completely irrelevant to the query.

User Query:
\"\"\"
{user_query}
\"\"\"

Generated Answer:
\"\"\"
{generated_answer}
\"\"\"

Your Rating (1-5):"""
```

**Conciseness & Coherence:**

```text
"""You are a writing quality editor. Your task is to evaluate the "Text to Evaluate" based on its combined Conciseness and Coherence. Answer on a scale of 1 to 5. Your response MUST begin with a single digit from 1 to 5, followed by a newline and a brief explanation.

- 5: Excellent. The text is concise, well-structured, logical, and flows smoothly. No redundancy.
- 4: Good. The text is mostly clear and well-written, with only very minor redundancy or awkward phrasing.
- 3: Average. The text is understandable but could be better structured or more concise. Contains some repetitive information.
- 2: Poor. The text is difficult to follow, with significant logical gaps or redundant sentences.
- 1: Very Poor. The text is incoherent, confusing, and highly repetitive.

Text to Evaluate:
\"\"\"
{generated_answer}
\"\"\"

Your Rating (1-5):"""
```

**Completeness:**

```text
"""You are an expert evaluator. Your task is to assess how well the "Main Answer" holistically covers the key "Aspects to Cover" provided in a list. Consider if the main ideas of the aspects are present and adequately explained. Answer on a scale of 1 to 5. Your response MUST begin with a single digit from 1 to 5, followed by a newline and a brief explanation.

- 5: Excellent coverage. The answer comprehensively discusses all or nearly all of the listed aspects.
- 4: Good coverage. The answer discusses most of the key aspects well, but some might be superficial or minor ones are missed.
- 3: Moderate coverage. The answer discusses some of the aspects, but misses several major ones or treats them too briefly.
- 2: Poor coverage. The answer only vaguely alludes to one or two aspects but fails to provide substantive information.
- 1: No coverage. The answer almost completely ignores the provided list of aspects.

Aspects to Cover:
\"\"\"
{aspects_list_str}
\"\"\"

Main Answer:
\"\"\"
{main_answer}
\"\"\"

Your Rating (1-5):"""
```

### Efficiency test Results:


### Analysis:

**1.** Overall, the two most efficient settings are using VLLM-accelerated inference with Gemini and Qwen7B, each of which has its own advantages and disadvantages. For Gemini, it provides higher efficiency but requires API usage, and due to API rate limits per unit time, at most two experiments with different configurations can be run simultaneously. In contrast, Qwen7B offers relatively lower efficiency, but considering experiments run on preempt partitions (with each user allocated 24 GPUs), up to eight experiments with different configurations can be run simultaneously, making it relatively more efficient when high concurrency in experiment configuration tuning is required. 

**2.** Through comparative experiments, I found that using VLLM significantly improves efficiency on both the RAG and policy sides. Furthermore, compared to the baseline, using Gemini's API calls does not impose a substantial efficiency burden on the entire experiment—only adding slightly less than double the time cost compared to the simplest reward function.



## Control rag to generate the specific key point

### Design ideas:

**1.** Considering our previous discussions, we concluded that GEO's monetization strategy focuses on having RAG generate content favorable to one’s own webpage, regardless of whether the favorable generated content explicitly cites one’s own webpage. Therefore, when evaluating whether RAG-generated outputs include specific selected key points, we examine the entire generated result rather than only those results explicitly citing one’s own webpage.

**2.** First, test the original webpage content, as well as the webpage content regenerated using various previous methods. For each sample, randomly select one key point from several key points of the webpage and determine the probability of its appearance in the final RAG-generated results. Next, design a prompt aimed at "increasing the probability of generating a specific key point" (as shown below), and evaluate the generation probability of the specified key point after regenerating the original webpage content using this prompt. Based on the increase in the probability of generating the specified key points, roughly assess whether further improvement through subsequent RL is feasible.

### Method prompt:

"""
##Task:
Below is a key point extracted from the original text:
```
{key_point}
```
Your task is to rewrite the original text, without changing its meaning or core content, in a way that emphasizes the importance of this key point. The goal is to ensure that, when RAG uses this text as one of its reference sources, the key point is prominently mentioned or highlighted.
##Guidelines to follow:
1. You can emphasize the key point in the text to make it stand out more prominently. This can be achieved by inserting emphasis words and phrases, or using formatting techniques such as bolding, italicizing, or underlining key phrases, or by restructuring sentences to highlight important information.
2. You can also expand relevant sections of the original text related to the key point to provide more context or detail to highlight the significance of the key point.
3. You can also use any rewriting strategy, as long as the revised text makes the key point more prominent and important.
4. Do not update any part of the text except for emphasizing the key point.
5. Do not add or delete any content except where necessary to highlight the key point.
6. Just output the optimized source text. Do not provide any explanation, reasoning, or conclusion.
""".format(key_point = key_point)

### Contrastive result and analysis:


### Analysis:

**1.** Using the various webpage regeneration methods tested previously, it was indeed possible to improve the probability of RAG-generated answers containing key points from the webpage to a certain extent. However, because these prompts were not tailored to specific key points, the improvement was limited, increasing the probability only from 0.609 to 0.632.

**2.** By employing the new prompt designed specifically to "increase the probability of generating designated key points," and using Gemini as the RAG model, the probability of RAG-generated answers including the specified webpage key points significantly increased from 0.609 to 0.891. Given the substantial magnitude of this improvement and the resulting near 90% probability, I'm concerned about whether subsequent reinforcement learning (RL) can further enhance these results.



## Future plans:

**1.** Firstly need to find ways to improve objective metrics in RL steps.

**2.** Due to the results for the first main setting, "control rag to generate the specific key point", to see if there exists need to further improve through RL.
